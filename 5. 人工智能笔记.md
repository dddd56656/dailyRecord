# 1. 常用指令

###### 1. 题目记忆

你是谷歌cto,能否帮我一个一个提问下面的这些问题?在我回答完一个之后，单独进行解析?要求符合谷歌标准格式。要求在此聊天中一直保持。

你是谷歌cto，帮我进行批改，找到不对的地方，要求符合谷歌标准



###### 2. 快速理解

你是谷歌cto，让我快速理解图片中的内容，要求符合谷歌标准，前提:我的智商极低，要求能够让我快速理解

你是谷歌CTO，还是无法理解，能否根据我给你的图片内容的公式内容，帮我进行理解?要求符合谷歌标准

###### 3.  过往后端和大数据两者的关系与理解

大数据项目实际上是为了利用传统数据库无法存储的数据的总和，相当于多源异构数据的采集、存储与处理，而相应的spark和flink则是专门用来处理海量数据，本质上和传统后端没有太大区别，是为了应对大量数据的存储和计算，所衍生出来的。

而前端更多的则是进行数据的展示。

人工智能则是更深度的方向，是针对图片，语义进行训练，且利用涌现现象的新的道路，我不知道人工智能会通往何处。

###### 



# 2. 机器学习与深度学习理论基础

## 2.1   机器学习分类: 监督，半监督，无监督

![image-20251011160923060](5. 人工智能笔记.assets/image-20251011160923060.png)

![image-20251011164601627](5. 人工智能笔记.assets/image-20251011164601627.png)

###### 简单来说，监督学习就是根据房子的地段，位置这些已知的标签来预测价格。无监督学习就是不知道有哪些标签，但是根据相似度进行分类。半监督学习就是在分类的基础上进行标签标记。

![image-20251011164230381](5. 人工智能笔记.assets/image-20251011164230381.png)

单变量线性回归



![image-20251011172037044](5. 人工智能笔记.assets/image-20251011172037044.png)

![image-20251011172129216](5. 人工智能笔记.assets/image-20251011172129216.png)





![image-20251011173627292](5. 人工智能笔记.assets/image-20251011173627292.png)



###### 简单来说，就是针对新系数的函数，但是得到系统的自变量的方式是通过损失函数进行求导得到系数的自变量，最后直到误差达到最小值，再往下走就会变大，来进行停止。

```txt
✅ **是的！你理解得已经非常接近核心本质了。下面我来帮你把这句话再“精炼+结构化”一下，形成一个彻底清晰的理解：**

---

## ✅ 简单来说（结构化表述）：

1. **我们想训练的是一个“预测函数”**：
   [
   \hat{y} = h_\theta(x) = \theta_0 + \theta_1 x
   ]

   * 系数 ( \theta ) 是这个函数的“控制参数”

---

2. **损失函数 J(θ)** 用来衡量：
   👉 当前这组系数（θ）到底预测得准不准

---

3. **我们不知道最佳 θ 是多少，所以设计了一个“系统函数” J(θ)**
   👉 J(θ) 是关于 θ 的函数，它的“最小值点”就是我们要找的最优参数

---

4. **怎么找到最小点？**
   👉 通过对 J(θ) 对 θ 求导
   👉 导数告诉我们“J(θ)往哪里减小最快”（即“下坡方向”）

---

5. **梯度下降做的事情就是：**
   [
   \theta := \theta - \alpha \cdot \frac{\partial J(\theta)}{\partial \theta}
   ]
   👉 沿着导数方向不断调整 θ，让 J(θ) 越来越小

---

6. **直到有一天：导数 ≈ 0，或者 J(θ) 不再明显下降 ⇒ 达到最小值 ⇒ 停止**

   * 此时的 θ，就是**你模型的最终参数**
   * 它能让模型预测误差最小 ✅

---

## ✅ 最终一句话总结你这句话的本质意思：

> 我们通过对损失函数 J(θ) 对 θ 求导，找出“调整系数的最佳方向”，一步步靠近误差最小点，直到再调整就会让误差变大 ⇒ 停止 ⇒ 得到最优系数。

---

你已经很接近工程师/研究员级别的理解了。

是否想要我用图像（例如 J(θ) 的 U 形曲线 + θ 下降轨迹）来再加强这个直觉？你说一句“上图”就搞。

```

![image-20251011172220528](5. 人工智能笔记.assets/image-20251011172220528.png)

###### 缩放是为了在多变量问题中，防止某个特征因为数值过大而主导模型训练，导致梯度失衡、收敛困难。

###### 非监督学习，份K-mean聚类和PCA降维

![image-20251011235408568](5. 人工智能笔记.assets/image-20251011235408568.png)

![image-20251011235706633](5. 人工智能笔记.assets/image-20251011235706633.png)

![image-20251011235456555](5. 人工智能笔记.assets/image-20251011235456555.png)

![image-20251011235724073](5. 人工智能笔记.assets/image-20251011235724073.png)

![image-20251011235517048](5. 人工智能笔记.assets/image-20251011235517048.png)

![image-20251011235534122](5. 人工智能笔记.assets/image-20251011235534122.png)

![image-20251011235557478](5. 人工智能笔记.assets/image-20251011235557478.png)



###### 半监督学习，简单来说，通过少量标注进行模型训练的算法，例如Self-Learning+Noisy-Student

![image-20251011235816049](5. 人工智能笔记.assets/image-20251011235816049.png)

![image-20251012000546168](5. 人工智能笔记.assets/image-20251012000546168.png)

![image-20251012000438978](5. 人工智能笔记.assets/image-20251012000438978.png)

![image-20251012000450851](5. 人工智能笔记.assets/image-20251012000450851.png)

![image-20251012000503580](5. 人工智能笔记.assets/image-20251012000503580.png)



###### 神经网络，简单来说，就是输入 经过单个或者多个函数 输出，激活函数则是条件限制

![image-20251012193854377](5. 人工智能笔记.assets/image-20251012193854377.png)



![image-20251012193924734](5. 人工智能笔记.assets/image-20251012193924734.png)





![image-20251012193953029](5. 人工智能笔记.assets/image-20251012193953029.png)



![image-20251012194006026](5. 人工智能笔记.assets/image-20251012194006026.png)

###### 简单来说，全连接就是上一层所有神经元和下一层所有神经元都可以互相连接

![image-20251012194447390](5. 人工智能笔记.assets/image-20251012194447390.png)

![image-20251012194605428](5. 人工智能笔记.assets/image-20251012194605428.png)

###### 前向传播（Forward Propagation）**：输入数据，层层计算，最后输出预测结果。反向传播（Backward Propagation）**：根据预测误差，借助优化器（如梯度下降）来更新参数（权重和偏置）

![image-20251012194710382](5. 人工智能笔记.assets/image-20251012194710382.png)

![image-20251012194941573](5. 人工智能笔记.assets/image-20251012194941573.png)

###### 前向传播得出结果，后向传播则是根据优化器得到新的系数

###### 

![image-20251012195003861](5. 人工智能笔记.assets/image-20251012195003861.png)

![image-20251012195022542](5. 人工智能笔记.assets/image-20251012195022542.png)

![image-20251012195042356](5. 人工智能笔记.assets/image-20251012195042356.png)

###### 深度学习，简单来说，输入，经过卷积层提取特征值，经过池化层压缩，到全连接层分类。卷积则是通过卷积核进行矩阵乘法宽度自然变小。

![image-20251012195606672](5. 人工智能笔记.assets/image-20251012195606672.png)

![image-20251012195733956](5. 人工智能笔记.assets/image-20251012195733956.png)

![image-20251012195749381](5. 人工智能笔记.assets/image-20251012195749381.png)



![image-20251012195807475](5. 人工智能笔记.assets/image-20251012195807475.png)

![image-20251012195821983](5. 人工智能笔记.assets/image-20251012195821983.png)







![image-20251012195841739](5. 人工智能笔记.assets/image-20251012195841739.png)

![image-20251012195942490](5. 人工智能笔记.assets/image-20251012195942490.png)

![image-20251012200006835](5. 人工智能笔记.assets/image-20251012200006835.png)

![image-20251012200024999](5. 人工智能笔记.assets/image-20251012200024999.png)

![](5. 人工智能笔记.assets/image-20251012200219054.png)



![image-20251012200554717](5. 人工智能笔记.assets/image-20251012200554717.png)



![image-20251012200237593](5. 人工智能笔记.assets/image-20251012200237593.png)





![image-20251012200249325](5. 人工智能笔记.assets/image-20251012200249325.png)

![image-20251012200311666](5. 人工智能笔记.assets/image-20251012200311666.png)

![image-20251012200331116](5. 人工智能笔记.assets/image-20251012200331116.png)



![image-20251012200342323](5. 人工智能笔记.assets/image-20251012200342323.png)



![image-20251012200358187](5. 人工智能笔记.assets/image-20251012200358187.png)



![image-20251012200415283](5. 人工智能笔记.assets/image-20251012200415283.png)



![image-20251012200441986](5. 人工智能笔记.assets/image-20251012200441986.png)





# 3：大模型发展全景与关键技术

###### Transformer架构的诞生与应用。

![image-20251014192242664](5. 人工智能笔记.assets/image-20251014192242664.png)

![image-20251014192255596](5. 人工智能笔记.assets/image-20251014192255596.png)



![image-20251014192432801](5. 人工智能笔记.assets/image-20251014192432801.png)



![image-20251014192446235](5. 人工智能笔记.assets/image-20251014192446235.png)



![image-20251014192503581](5. 人工智能笔记.assets/image-20251014192503581.png)



###### NNLM 的目标是，建模语言概率，预测下一个词,但在这个过程中得到了词向量的概念，为后续模型的发展打下了思路。NNML则是通过输入词向量，经过tanh非线性变化，最终通过softmax得到预测结果。





![image-20251014192517821](5. 人工智能笔记.assets/image-20251014192517821.png)



![image-20251014192546205](5. 人工智能笔记.assets/image-20251014192546205.png)

###### RNN本质是根据当前输入词向量和上一个隐藏状态一起计算出 当前隐藏状态。

![image-20251014192608020](5. 人工智能笔记.assets/image-20251014192608020.png)

###### 本质上，LSTM通过不同的系数对不同的内容进行增删





![image-20251014192628647](5. 人工智能笔记.assets/image-20251014192628647.png)



###### 注意力模型，





![image-20251014192646462](5. 人工智能笔记.assets/image-20251014192646462.png)

![image-20251014192752247](5. 人工智能笔记.assets/image-20251014192752247.png)

![image-20251014192821530](5. 人工智能笔记.assets/image-20251014192821530.png)



![image-20251014192845871](5. 人工智能笔记.assets/image-20251014192845871.png)



<img src="5. 人工智能笔记.assets/image-20251014192912483.png" alt="image-20251014192912483" />







![image-20251014192939820](5. 人工智能笔记.assets/image-20251014192939820.png)







![image-20251014192951510](5. 人工智能笔记.assets/image-20251014192951510.png)



![image-20251014193014566](5. 人工智能笔记.assets/image-20251014193014566.png)



































